name,document
Gendershades_Analysis.pdf,"Gendershades - Intersectional Accuracy Disparities in Commercial Gender Classification  Description: A study highlighting discrimination on the basis of skin color and gender in commercial face recognition algorithms. As seen in: New York Times, Ars Electronica, Time, Fortune etc. Victims: Women, especially women with darker skin tones. Providers: IBM, Microsoft and Megvii.  What's the bias and how was it discovered? While trying to get a facial recognition software to recognize her face, Joy Buolamwini              noticed that the system performed differently depending on who stood in front of the              camera. When she stood in front of the camera, the algorithm couldn't determine that              her black face was in fact a face. Yet, when she wore a completely white mask, the                 software immediately marked it as a face. A few years later, as a graduate researcher at                MIT, she co-published her findings on biases of three commercial face recognition            systems in a paper entitled Gendershades - Intersectional Accuracy Disparities in           Commercial Gender Classification.1 At the time of publication, face recognition software by IBM, Microsoft and Megvii was              significantly worse at identifying darker skinned people compared to lighter skinned         "
Allegheny_Family_Screening_Tool.pdf,"Tool Bias Towards Socio-economically Disadvantaged  In Pennsylvania county Allegheny, the Allegheny Family Screening Tool (AFST) was lunched for risk for children modelling. The Allegheny County Department of Human Services (DHS) used the tool to enhance the child welfare call screening decision-making process1. There are more than 15.000 referrals per year made to child protection agency in Allegheny and with 30 officials, it is difficult to make efficient and accurate decisions on each referral call2. AFST, on the other hand, analyses data for individual involved in an allegation of child maltreatment stored in the DHS database. The result of the algorithm is a 'Family Screening Score' (from 1 meaning the lowest to 20 the highest risk) that predicts the likelihood of social care worker intervention3. DHS combines the algorithmic insights with traditionally gathered information and predicts the likelihood of a child's removal from the home. According to the DHS, the information summarized by the score does not replace clinical judgment but rather provides additional information to assist in the call screening4.  However, the New York Times Magazine article an Algorithm Tell When Kids Are in Danger?' revealed that despite its efficiency, the tool discriminates based on income, geographic location and race5. First, the model relies on public data on usage of public service and as families who used such services are generally poor, the algorithm unfairly penalizes less prosperous families by subjecting them to more scrutiny6. Secondly, the geographical location is not represented in the data set (most families accessing public services in Allegheny County live in dense urban neighbourhoods). Thus, a family living in relative isolation in a well-off suburb is much less likely to be reported to child abuse or neglect hotline than one living in crowded housing conditions7. Finally, the AFST model is based on data"
Hotel_Stars.docx,"they fined Google 1.1 million euros for displaying misleading rankings of French hotels on the Google search engine and Google maps[footnoteRef:1]. French hotels along with the French union of hotels and restaurants accused Google of harming both hotels and consumers by replacing the French star ranking system of Atout France with Google's own ranking system. According to the French hotels and their representatives, not only did Google's system harm hotels by presenting them as lower-ranked than they were in Atout France's official classification, but consumers were also misled about the quality of services they would receive at a hotel incorrectly ranked by Google. The trust of consumers and hotels alike was broken by Google's faulty classifications. While computational rankings often enjoy an aura of impartiality and superiority based on their use of opaque mathematical calculations often indecipherable by laypeople, consumers and hotels in France prefer the decidedly human oriented ranking system of Atout France over its more technical Google counterpart.  [1:  https://www.politico.eu/article/google-fined-in-france-for-misleading-hotel-ranking-search-engine/ ] Google Ireland and Google France have shifted to using Atout France's ranking system when displaying how many stars a hotel has. Google has also agreed to pay the French fine. Now that Atout France's ranking have been restored on Google search and maps, consumers can select a hotel based on the official standard. The standard benefits the consumer because it can't be influenced by online growth hacking tactics, which contributes to the higher level of trust in Atout France's ranking system over Google's.Why did Google try to circumvent Atout France, the established gatekeeper of the French hotel industry? There seems to be a lack of acceptability as to why the results of Google's ranking system do not align with the French industry standard since misleading consumers to believe the Google ranking is equivalent to the"
Detect_Sexual_Orientation.pdf,"two Stanford University researchers published a study entitled ""Deep neural networks can detect sexual orientation from faces"". Yilun Wang was a graduate student and Michal Kosinski an associate professor. In their thesis they explain that they created an algorithm that can distinguish the sexual orientation (homosexual or heterosexual) of a person using its picture with an accuracy of 81% right answers for men and 71% for women. The facial features used by the classifier ranged from fixed features (the shape of the nose for example) to transient features (like the grooming style). 35,326 facial images were used as training data for the algorithm. The two researchers wanted to test the hypothesis that people's private traits (like sexual orientation) are exhibited on their face but that others cannot comprehend them. Issues and controversy The two researchers have claimed that their goal was to test the Prenatal Hormones Theory (PHT) stating that the prenatal hormones level impacted a person's physical features. They also wanted to prove that making such algorithm was possible and make others aware of the threats it caused1. Some scientists supported their work, claiming that photos of a face are not neutral and will naturally provide some level of clues about sexuality or social class for instance2. They have also been supported by some medias such as LGBTQ Nation3, but the paper mainly caused outrage.  The controversy started when the first draft of the thesis was published. Human LGBTQ+ rights protection groups such as GLAAD or Human Rights Campaign immediately called out the dangers linked to the algorithm. The main concerns were that such technology posed a safety and a privacy issue for LGBTQ+ and non-LGBTQ+ people. The algorithm was also criticized online by scientists and academists who saw it as a revival of physiognomy (a discredited theory"
Predictive policing and discrimination.pdf,"last decade, a variety of press releases discussed the discriminatory nature of many algorithmsgoverning our societies.1 Among such reports, one of the most discussed topics was the use of predictivealgorithms by police departments. Predicting crime has always been part of police work. However, sincethe 1990s, police officers started doing so systematically through the use of computers and algorithms.2 In the last decades, the use of predictive policing techniques spread through many countries. Data-driven programs in police departments aim to stop crime before it occurs. Despite its widely shared goal,predictive policing software has raised serious concerns among activists, scholars, and police chiefs.This case study focuses on one of the biggest companies developing tools for data-driven policing:PredPol. In the last years, the PredPol software has received millions in venture capital funding and isnow used by more than 50 police agencies in the US and UK 3. PredPol grew out of a research projectbetween the Los Angeles Police Department and UCLA.4 The choice was made due to the greater quantityof empirical evidence about its functioning. However, the following reflections apply to the majority ofthe predictive policing systems.ControversiesAt their core, all predictive policing programs work similarly: they use historic crime data to break downsome areas of the city into blocks in order to pinpoint locations that are likely to be the scene of futurecrimes. Today, the majority of these services are machine learning systems that identify correlationsbetween the environmental variables related to the crime reports (the time, the zone, the season in whichthe crime happened...). Consequently, the police departments usually patrol more intensively the spacialand time slots correlated with a higher number of crime reports. The available literature shows that theimplemented techniques have a discriminatory potential caused by two main sources: biased police dataand confirmation feedback loops (Richardson et al. 2019, pp. 40-47).As regards"
Gaydar.docx,"2017, two researchers at Stanford University, Michal Kosinski and Yilun Wang carried out a study investigating the potential of facial recognition algorithms in identifying the sexual orientation of people. The study was published in the Journal of Personality and Social Psychology and utilised a large sample of 35,000 facial images acquired from a US dating website. Methodology 'Deep Neural Networks (DNN)' were used to extract the facial features, mainly focusing on the ""non-transient facial features, disregarding the head's orientation and the background"" (Wang & Kosinski, 2018) in order to detect differences in facial features which are too subtle to be identified by the human eye. This was followed by using logistic regression in order to identify the correlations between the features and the sexual orientation self-reported by the person on the dating website (The Economist, 2017). Finally, the results revealed that the ""faces of gay men and lesbians had gender-atypical facial morphology, expression, and grooming styles as predicted by the Prenatal Hormone Theory (PHT)"" (Wang & Kosinski, 2018), hence providing strong support for the PHT, highlighting that people are gay by birth and it is not a choice. Looking at the performance of the algorithm, it can be understood that given the dataset and inputs available, it is still performing quite well, as ""when shown one photo each of a gay and straight man, both chosen at random, the model distinguished between them correctly 81% of the time. When shown five photos of each man, it attributed sexuality correctly 91% of the time. The model performed worse with women, telling gay and straight apart with 71% accuracy after looking at one photo, and 83% accuracy after five"" (The Economist, 2017). The Backlash and Potential Threats There was an instant backlash towards the unethical nature of the study, especially by the"
Border_Control.pdf,"EAP  100164344 Aligning to the prevalent trend of public administrations turning towards artificial intelligence systems with an eye to improving efficiency, cost-effectiveness and fairness, the EU opted to fund a biometric border surveillance programme (iBorderCtrl) beginning September 2016 (CORDIS European Commission 2020). To date, the iBorderCtrl project has absorbed just over EUR4.5 million in EU funds (idem) with little transparency regarding its functioning or merits.  The EU's migration numbers from 2018 reported 2.2 million immigrants entering the bloc (European Commission 2021) while over 700 million people cross the borders annually. The pressures on external border controls have contributed to the degradation of security protocols in favour of minimising disruption. Within this context, the iBorderCtrl project was launched in co-operation with security agencies of several EU member states (namely, Greece, Hungary, Latvia, Poland, Spain). The pilot programme ran between September 2016 and August 2019, fully funded by the EU, in the hopes of deploying automated border. According to the EU Commission, iBorderCtrl is envisaged to leverage new technologies to render border control more efficient and quicker for third nationals while improving security controls of the Schengen Area (CORDIS European Commission 2020). iBorderCtrl is a two-fold procedure. First, a pre-registration process by the traveller on an online platform gathering their personal, travel and vehicle data. Next, by virtue of webcams, the traveller is subject to an interview with a digitalised border guard (""avatar"") utilising innovative technology to detect deception by analysing the micro-gestures of travellers (this includes facial recognition technology, eye tracking technology, analysis of micro-reactions, etc.) (European Commission 2018). The AI system analyses the interview in real-time and cross checks the collected data against outstanding security databases to whittle out immediate security threats before designating the degree of risk of the traveller (low, medium, and high) (Idem). Second, at"
Exam_Algorithm_UK.pdf," Assignment Discriminatory Exam Algorithm in the United Kingdom  1 Quinn and Adams, ""England Exams Row Timeline: Was Ofqual Warned of Algorithmic Bias?"" 2 Introduction: Context & Algorithm In the following, an algorithm to standardize and estimate grades for graduating students from the UK is explored and analyzed. Various articles from Forbes2, BBC3 and the Guardian4 serve as information sources for this analysis.  In the UK, students are assessed with the GCSE and A-level exams for graduation. Due to the Covid-19 pandemic these final exams could not take place in 2020. In order to provide final evaluations to the affected student regardless, the UK government official exam regulator Ofqual introduced an algorithm to calculate a grade for each student. This algorithm is based on teacher estimations and past performance data of schools and students. Its aim is to standardize the grading across the country and prevent too generous and unrealistic grades estimated by teachers alone. When the algorithm was introduced in August almost 40% of the final grades were below the teacher's estimations and thereby downgraded by the algorithm. Eventually, this led to objections of students, parents, teachers and public officials. On August 17, the UK government changed the results and excluded the algorithmic calculations in order to only reflect the original teachers estimates.  The algorithm was fed with different data points for each student. Their teachers submitted an estimated individual grade, a ranking of this grade for the whole school and previous individual exam results. In addition, information on the school's performance for each subject over the last three years was added to the algorithm. The underlying"
Crime_Prevention_Algorithmic_Bias.pdf,"Bias and Crime Prevention In 2013, Chicago Tribune has shared a story of a young black man Robert McDaniel who has received an unannounced visit by the local police department that has warned him not to commit any future crimes. This has caught McDaniel by surprise since he has never been involved in any criminal activity. However, the algorithm used by the local police department has marked his neighbourhood alongside several other ones as a hotspot for potential criminal activity and these police visits were considered a prevention1. The idea was that such prediction would lead to a better allocation of police resources as those places which are more prone to crime would be assigned more police patrols. What is problematic, however, is the fact that the low-income and non-white residential areas have been identified as the most likely to experience crime2. The Royal Statistical Society and PredPol The same kind of systematic bias and discrimination had already been identified and proven in 2010 by the Royal Statistical Society (RSS) which examined crimes related to drug use in Oakland, California. It has compared the 2011 National Survey on Drug Use and Health (NSDUH) with police records on drug use within the city3. The PredPol algorithm, used in the RSS experiment as well as in the above-mentioned Chicago case , has been developed by the LAPD to identify, learn and then reproduce patterns in data4. It is based on the assumption that certain crimes tend to focus in time and space5 and the algorithm has been portrayed as race-neutral, excluding any personal information and reliant on only three data points (past type of crime, place of crime, time of crime). PredPol has been offering one-day-ahead predictions of crime rates across various parts of the city. Those areas with the highest predicted"
Monitoring_Patients.odt,"risk care management>> biased by an algorithm:	To begin, this algorithm was conceived to help hospitals and insurance companies identify which patients would need to benefit from extra primary-care visits for closer monitoring, providing chronically ill people by specially trained nursing staff in order to limit serious health complications, reducing costs, as well as increasing patient satisfaction. That is why the algorithm was designed using previous patient's health care spending as a proxy for medical needs. However, this proxy was biased: Black and White people did not have the same level of need, while having the same level of spending. On top of that, Black patients had to pay more active interventions, especially emergency for diabetes or due to hypertension complications. That is to say the algorithm incorporated a historical discrimination to give some predictions: Black people had lower risks care scores, and so were presented artificially in a good health than White citizens. The National Academy of Medicine (NMA) denounced the health care system of discriminating Black American citizens:	In 2005, the NMA firstly denounced the strong correlation between race and mortality, and so the lack of access to health care by Black citizens. Moreover, in 2019, a paper research from Z.Obermeyer, B.Powers, C.Vogelu and S.Mullainathan1 Ziad Obemeyer, Brian Powers, Christine Vogeli, Sendhil Mullainathan, Dissecting racial bias in an  algorithm used to manage the health of populations, Obermeyer et al., Science 366, 447-453 (2019) , proved that among 200 million people, modifying the algorithm conception would increase the percentage of Black people receiving extra help from 17,7% to 46,5%. A claim for a right to live and to an equal treatment as White people in the end. In fact, focusing on the calibration bias, they showed that at a given level of health, Blacks generate lower costs than Whites, even"
Bias in AI Hate Speech Detection_Essay.pdf,"Biases in AI Hate Speech Detection With the rising popularity of social media and online communication, hate speech and virtual harassment developed into a problem platforms have to deal with. As the amount of hateful comments is increasing on a daily basis, AI tools designed to detect and flag inappropriate, offensive and spiteful remarks seem to promise a solution. However, these algorithms can also work contrary to their intended effect and be deeply biased towards different groups of people.  In August 2019, Forbes reported about such problematic biases in an AI algorithm developed by Google.1 The article refers to a study conducted by a group of researchers at the University of Washington, who found that the algorithm was marking tweets posted by African Americans as hate speech. The AI tool was created by Google and its subsidiary Jigsaw and trained by going through a database of 100'000 tweets Google's API ""Perspective"" had labelled as ""toxic"". Based on this, the algorithm was able to evaluate and identify new content on a scale from ""toxic"" to ""healthy"". As is elaborated in another article on the issue by Fortune Magazine, the scientists conducted their study by comparing the algorithm's performance to the one of human annotators on the same dataset.2 They found that both humans as well as the algorithm labeled tweets commonly associated with African American vernacular, which often contains curse words that other social groups may find offensive but that are typical to this dialect, as offensive or hateful. However, once human annotators were given the information that African Americans wrote the tweets, they were much less likely to classify them in this way.  According to the scientists, this shows the algorithm's limits in understanding nuances of human language and its failing of taking into account relevant ethnical, societal"
The Case of SyRi.docx,"case studyThere have been several high-profile cases of blatant discrimination by governmental algorithms in the recent years, and one of the most widely cited is the example of the Dutch government's 'System Risico Inidcatie' or SyRI system.[1-4] Implemented from 2014 onwards to tackle welfare fraud, SyRi induced a forceful backlash from large sections of the Dutch public and civil society, and was eventually taken to Court by a consortium of NGOs, resulting in what many have described as the 'first time an algorithm was taken to court and lost'.[5] The Hague District Court ordered the cessation of the use of SyRi primarily on the basis of Data Protection and Privacy grounds, however, SyRi was also widely criticised for its disparate treatment of residents in poorer neighbourhoods.[6-8] This paper will provide a brief analysis of the SyRi case. In doing so, it will focus mainly on the issue of SyRi's algorithmic potential for (un)fairness. However, given the centrality of obtaining and using data for the effective functioning of predictive algorithms, at times, it will be necessary to make references to the data protection and privacy aspects of the case. This paper is structured into three parts. The first part outlines the context of the case, including its main actors and their key arguments. The second part will look at the failures of the SyRi Policy, with a particular focus on the moral principles the use of the algorithm has violated. The last part will provide a brief discussion of the ongoing theoretical debates related to this case, followed by a few personal suggestions for what could have improved the fairness of SyRi. The Case: Main Actors and ArgumentsSyRi was introduced by the Dutch government in 2014. Its objective was to 'prevent and combat fraud in the area of social security, taxes"
Gender_Shades.pdf,"- 58661  A review of Joy Buolamwini's landmark ""Gender Shades"" study  Framework   In 2016, the then MIT PhD student Joy Buolamwini founded the Algorithmic Justice League (AJL), a digital advocacy organization that aims to raise awareness of the potential social and racial biases induced by artificial intelligence (AI). A computer scientist and MIT MediaLab researcher, Buolamwini launched this initiative after she put in light in her Master thesis (2017) a fault in commonly used face recognition algorithm, which failed to recognize dark skinned people's faces as her own.   The AJL seeks to fight algorithmic bias under all of its forms and to sensitize society at large on this issue. It mainly focuses on biases encoded in facial recognition algorithms that Buolamwini refers to as a ""coded gaze"", i.e. ""the embedded views that are propagated by those who have the power to code systems"" (2017). Buolamwini further investigated the topic in a second paper published in 2018. The paper, which puts forward recurring misgendering errors among dark skinned females, soon became a landmark study in the field of algorithmic bias research. This essay aims to discuss the latter's findings.  Why biases in facial recognition algorithms matter   Facial recognition is increasingly used in fields as diverse as airport security, street surveillance, smartphone applications and payment software. Facial recognition is a subdomain of computer vision that investigates computer's capacity to identify objects (human and non-human). Mislabeling female faces as male faces raises both technical and ethical issues: a technical issue because mislabeled data can lead algorithms to erroneous conclusions; and an ethical issue as algorithm designers should make sure not to generate gender or race discrimination.   The experiment    The study ""Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"""
Facebook_s discriminatory advertisement algorithm.pdf,"01-03-2021 Discriminated and discriminating without even knowing: how Facebook's ad delivery algorithm contributes to structural discrimination ""Senator, we run ads"", Mark Zuckerberg replies, unable to prevent a grin appearing on his face, after U.S. Senator Orrin Hatch asked him how Facebook can sustain a business model in which users do not pay for their service.1 Zuckerberg's witty - and truthful - reply might have put a smile on more faces than just Zuckerberg's, Facebook's advertisement mechanisms are everything but a joke. AlgorithmWatch found that Facebook's advertisement algorithm uses ""gross stereotypes to optimize ad delivery"", thus reinforcing structural discrimination.2 The aim and order of this paper is to, first, provide insight in the specifics of this case, second, to analyze the key stakeholders involved, third, explain the moral principles being broken, and fourthly, to address four possible solutions aiming to prevent structural discrimination by Facebook's advertisement algorithm and similar algorithms in the future. How Facebook optimizes its ad delivery in discriminatory ways Building on research on how Facebook's advertisers can exclude users by race when selecting their target groups, AlgorithmWatch wanted to go a step further and research the functioning of Facebook's ""second round of optimization"", which entails a second round of targeting, executed after advertisers submitted their own preferences.2 AlgorithmWatch advertised job offers on Facebook for machine learning developers, truck drivers, hairdressers, childcare workers, legal counsels, and nurses. All their ads were written in masculine form and contained a picture related to the job. The key part of their research design? None of the ads were targeted other than at their respective geographical areas. Where the researchers did not expect their ads to be shown completely fifty-fifty to men and women, they did expect to reach roughly the same audience. Nothing was less true. For example, in Germany, the ad"
Amazon's sexist AI hiring algorithm.pdf,"Midterm Essay: Amazon's sexist AI hiring algorithm   Until late 2018, Amazon had been using a state-of-the-art AI recruiting tool. It streamlined their hiring process in order to hire the best of the best whilst wasting as little time as possible in the candidate-searching process. Many major companies use such algorithms, especially for screenings. In October 2018 however, Reuters broke the news1 that the top-notch AI system actually showed clear bias against female applicants. This discrimination was particularly strong in positions of software development, as well as most technical positions.   I found Amazon's AI blunder to be a very interesting case study of an AI project where algorithms, as well as computational agents are responsible for discriminating against women.   How did this happen? A quick analysis  As soon as Amazon realized their AI discriminated against women, they scrapped the system and dealt with the problem internally, keeping PR communication at a minimum. However, we now know that the main reason for this misadventure is that Amazon's developers had trained their AI system based on models dating back over a 10-year period. Therefore, the recruiting tool had been trained to vet applications by observing the existing patterns of resumes which had been submitted to the company in the past decade. It comes without surprise that, in the 2010's, the tech industry was (and still is) dominated by men. Therefore, most applications used by the AI for training were from male applicants, a sad reflection of the vast male dominance over the industry. As such, Amazon's AI system taught itself that men were ""preferable"". It thus penalized resumes which included the word ""women"" (i.e., ""women's chess club captain) and downgraded applicants who had graduated from all-women's colleges.    Who was involved and who is"
Self-driving cars.docx,"systems' biases: the case of level 4 and 5 self-driving cars' recognition of pedestrians""Decades of fights for civil rights and equality have been unwritten in a few lines of code"", affirms EqualAI executive director Miriam Vogel[footnoteRef:1]. Thus, facial recognition technology in machine vision systems recognizes white men more easily than women and/or minorities' features. While this could be anecdotical in many decision-making related algorithmic applications, such irregularity can be decisive in life-threatening situations. Self-driving cars are the most striking example, as the potential for destruction is immense and since they are widely used. We are here considering level 4 and 5 self-driving cars, entirely based on automation and artificial intelligence (AI). Indeed, there is no human driver involved in their piloting, all are passengers while the AI is taking charge of the driving. Indeed, although only one pedestrian death has been recorded in the press since self-driving cars have been allowed on national roads[footnoteRef:2], depending entirely on automated systems and algorithms, their quick development led me to research on pedestrians' unequal vulnerability when faced to self-driving cars. Indeed, according to a research from Georgia Institute of Technology, ""a white person is 10% more likely to be correctly identified as a pedestrian than a black person""[footnoteRef:3]. [1:  Conference << Boundary Breakers: Women Driving The Future of Tech >>, Brooklyn, N.Y.]  [2:  Wakabayashi, D. << Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam >>, The New York Times, March 2018.]  [3:  Wilson, B. Hoffman, J. and Morgenstern, J. Predictive Inequity in Object Detection, 2019.] Several stakeholders are involved in this discriminatory outcome - causing or enduring it. On the one hand, automakers are focused on ensuring that the car drives, and AI engineers on creating the most precise decision process to make the driving accurate"
Amazon_hiring tool.pdf,"dominant presence as the largest online retailer has its own challenges in terms of hiring large number of employees. Automation being one of the key tools to the company's growth, a similar approach was to be taken with respect to hiring. Amazon in 2014 set up a team, based in Edinburgh, to develop a hiring tool powered by artificial intelligence. The objective of the tool was to simplify the process of hiring and ensure that the ""best"" candidate for the job was chosen.  The tool was to scrape the web for candidates and give a rating to individuals in the same five-star format that amazon products are subjected to. This would simplify the process so as to review the most apt candidates for subsequent rounds. It was soon discovered that the tool was discriminatory towards women in particular. The algorithm began to penalize candidates based on keywords such as ""women"" from ""women's chess club captain"". Although the engineers attempted to rectify such an issue, the algorithm was found to circumvent these and reform discriminatory biases. Candidates of two all-girls schools were down voted even though they had never mentioned their schools. The tool thus showed female candidates in an unfavorable light, preferring male oriented resumes.   Amazon soon shut down its division in 2015 and stated that no hiring process was carried out using the tool. The problems were beyond gender bias as they noticed that unqualified candidates were recommended for the job. However, a more elementary version of the tool was used to handle simpler tasks later on (such as removing duplicate candidates). Amazon is hoping to approach the tool once again as of 2018, this time focusing on diversity.  Key takeaways The tool was not generic, the team created 500 models so as to suit"
biased_liberal_media.pdf,"Conservatives: The Right (in) Numbers The January 6th US Capitol insurrection attempt by far-right militias, Donald Trump supporters and Skyrim cos-players, as well as the subsequent Trump's twitter ban have revitalized inquiries into the role that social media platforms play in democratic processes. Until now regarded as refuge and breeding ground for populist right-wing politics, big tech corporations like Google, Facebook and Twitter have been added to the ""biased liberal media"" hit-list following republican allegations of unfair content curation. With algorithms and machine learning technology being the primary tools of mass social media management, they too are at the heart of the rather bipartisan concern over the covertly-totalitarian power that big tech companies exert over agenda-building. In the past few years Facebook, Instagram and Google's algorithms have been shown to produce racist, sexist and classist results, spawning a wealth of research on ""algorithmic fairness"", but continued lack of transparency, access and understanding about the technologies at play have made it an easy target for anxiety-farming politics. More recently republicans have launched campaigns against perceived unfairness and preferential treatment for their political opponents, with Senators Ted Cruz and Josh Hawley urging the Federal Trade Commission to investigate content curation that allegedly censors right-wing ideas. Are their frustrations warranted, or is it another falsehood instrumentalized for political gains? What role do algorithms play in partisan politics? 90% of US republicans believe it is likely that social media sites censor political viewpoints and are way more likely than democrats to believe that their political opponents are favored instead (Pew, 2020). Some examples of alleged censorship include Twitter's decision to deploy automated labels that notify users if a post didn't pass a fact-check or violated terms of use, as well as the ominous sounding ""shadow-ban"" feature that supposedly silences conservative spokespersons. Facebook too has"
AI biases in the criminal justice system.docx,"Criminal Justice SystemI-Main actors involved: Who is denouncing whom?Risk assessments scores are increasingly common in courtrooms across the US. These modern risk tools were originally designed to provide judges with insight into the types of treatment that an individual might need, from drug treatment to mental health counseling. But today these algorithms are used to help judges predict the level of danger that a defendant could represent if he were to be set free. Therefore, these algorithms influence judge's decisions at every stage of the criminal justice system by deciding who can be set free before a trial, or by assigning sentences or parole. The most common algorithm that is used today to rate defendants' degree of violence and to define their sentence is the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) algorithm. The Justice Department's National Institute of Corrections has been encouraging in the last years the use of such automated assessments at every stage of the criminal justice process. A reform bill mandating the use of such assessments in federal prisons is also pending in the US Congress. However, in 2014, then U.S. Attorney General Eric Holder had warned that risk scores generated by AI algorithms might be injecting bias into the courts, by exacerbating unjustified and unfair disparities that are already very widespread in the US criminal justice system, such as the color of the skin or the wealth of the individual. Yet the sentencing commission did not launch any study that could assess the validity and the accuracy of these of risk scoresAttention to the real efficiency of these algorithms only arose when sentences made by judges in different American states which authorized the use of risk assessments to dictate a sentence, started to be made public. There is an exhaustive list of cases. For"
Teenage_Pregnancy.pdf,"de Intervencion Social: sexism and hidden interests Teenage pregnancy is an issue treated differently by the different communities around the world. The               province of Salta, in Argentina, decided to turn to AI to generate effective policy to try to change this                   reality. In partnership with Microsoft, the province decided to work on a public-private partnership in               which the government would provide the datasets on the target population of the project and the                company would provide the software to be used in the operation. The datasets provided by government                on the target population gathered data of girls and young women between 10 and 19 years old, including                  age, country of origin, ethnicity, educational level of the household head, neighbourhood, mental and              physical disabilities, number of people in the household, and the availability of running hot water within                the household1. The goal of this project was to identify the likelihood of this population to undergo                 teenage pregnancy before it occurs, and provide directed action to"
Optum's Health Care Algorithm.pdf,"In late 2019, numerous media outlets reported that artificial intelligence software widely used by American hospitals had led to racial discrimination in the provision of health care. The findings came from a study, published in Nature, which revealed that black patients were less likely than equally sick white patients to be allocated additional care resources. The culprit identified was an algorithm, sold by health services company Optum, and used in managing the care of about 200 million Americans.1 The algorithm consistently underestimated the health care needs of black patients; if the racial bias had been corrected, the percentage of Black patients receiving extra medical attention would have gone up from 17.7% to 46.5%.2 Bias in the data The problem with Optum's algorithm was that it used cost as a metric to establish need for treatment. On face value, this makes sense: the higher a patient's historical cost of care, the more likely it is that this patient is very sick and needs specialised attention. And a cursory check of the data itself shows nothing unusual: average costs for black patients are about the same as average costs for white patients. The problem then? Black individuals are actually substantially sicker, on average, than white individuals. 3Clearly, the cost metric doesn't accurately represent black patients' level of sickness. Underlying inequities So why do black patients spend less on treatment? Because of unequal access to health-care. Higher rates of poverty among black households means that black patients use medical services less than their white counterparts.4 There is also a history, in U.S.medicine, of discrimination and racist practices towards black individuals. Black patients have often been used for medical experimentation without their explicit consent,like in the infamous Tuskegee study. A substantial amount of research in psychologyhas also documented differences in the perception of black patients by physicians, interms of intelligence, or or pain tolerance, for instance.5 As a result, Black patientshave"
